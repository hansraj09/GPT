# GPT

This is the full implementation of the GPT class based on the Transformer architecture from the paper "Attention Is All You Need".

<img width="256" height="653" alt="image" src="https://github.com/user-attachments/assets/28b2edc5-e7e8-44c4-81a0-4e6d6cb31965" />

While the GPT class implements the decoder part of the transformer as in the diagram, there are some minor changes, like using Norm & Add before Attention instead of after.

Based on the tutorials by Dev G on Youtube.
